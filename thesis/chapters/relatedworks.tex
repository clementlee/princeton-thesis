\chapter{Related Works}

In this work, we are primarily interested in optimizing neural network architectures and other hyperparameters.
Towards this end, we investigate current findings in the literature.
Neural network architecture self-optimization is a very new topic, and many results are preliminary or incomplete.
Nevertheless, they provide an important glimpse into the contemporary research space, and are highly motivational to the specific topic of this thesis.
The aim of this section is to cover some of the existing work that specifically focuses on network optimization, and to provide some grounding for our contributions.

\section{Parameter Deletion}
Ever since neural networks have been developed, experts have wondered how to make them more efficient.
The fixed initial structure required to train a network is one that is inherently overparametrized, because the minimum number of parameters needed is not known ahead of time.
Further exacerbating the problem, neural network training is often slow and requires significant computational power, limiting the ability to test out how altering the number of parameters affects the network results.
The natural solution is, therefore, to train an oversized network, and then whittle it down to size.
Initial practices were based on heuristic deletion; that is, algorithms that deleted all weights $w$ where $w < p$ for some low-pass filter $p$.
These methods generally result in a sparse network (where the network has missing connections), which are difficult to represent and operate on efficiently.

LeCun et al.'s early work from 1989, \emph{Optimal Brain Damage} \cite{lecun1989optimal}, showed that these heuristic-based methods were inefficient and could irreparably destroy a network.
They proposed a method based on error gradients that could more accurately find weights that can be removed with minimal perturbation to the final error.
By what is effectively the butterfly effect, the deletion of a weight with small magnitude can actually prove to have a significant impact on the network.
By taking the Taylor series of the error function to two terms, they show that it is possible to calculate the two-dimensional Hessian matrix representing the importance of each weight.
For reasons of efficiency, LeCun et al. approximate the Hessian with a diagonal matrix, and gradually remove the terms with the smallest saliency.
By retraining the network repeatedly after removing connections, they are able to show a significant improvement in performance despite requiring less parameters.

This was taken a step further by Hassibi et al. \cite{hassibi1993second} in their followup work, \emph{Optimal Brain Surgeon}.
By analyzing the Hessian matrix of typical networks, they show that the Hessian matrix is often nondiagonal, and that Optimal Brain Damage can often irreparably destroy small networks.
They utilize the full Hessian matrix to better understand the interactions between each pair of weights.
Hassibi et al.'s algorithm is among the most detailed methods shown to delete weights from a network, and they show that their algorithm is in fact optimal for specific small networks.
However, the calculation of the Hessian is an $O(n^2)$ operation in both space and time, making it largely unsuitable for networks in the modern age, where $n$ (the number of parameters) is often in the millions or tens of millions.

Within the last few years, an increasing amount of literature has been published on parameter deletion, especially as network complexity has grown at a pace that far outpaces the corresponding technological advancements.
Han et al. \cite{han2015learning} work on reducing existing architectures using a combination of heuristic-based deletion methods and weight regularization, and show that there is significant promise.
These methods are more common and rely on deleting weights, which can help optimize performance when running on CPUs.
They have limited application on GPUs and especially on convolutions.
To solve this problem, Hu et al. \cite{hu2016network} trim entire nodes and convolutions from the network, allowing better performance by fully removing them from the layers.
They improve on Han et al.'s results by producing networks that are smaller and more accurate, and are even able to see some extremely small improvements over the untrimmed network for some of the largest networks.
They hypothesize that this is due to optimizer efficiency.
However, we note that despite showing that it is possible to reduce parameter count significantly, no modern work has shown measurably better overall results.
These results are further supported by Murray and Chiang \cite{murray2015auto}, who utilize the same methodologies on natural language modelling and observe similar performance.
Instead, the goal is generally to minimize model size while keeping accuracy fixed (or reducing it slightly).
In this thesis, we aim to more efficiently utilize existing capacity and achieve improved results over the architectures we start with.

\section{Specialized Architectures}
Another direction researchers have exploited to minimize the number of parameters required is specialized network design.
Notable work in this field includes Squeezenet \cite{iandola2016squeezenet} by Iandola et al., which utilizes a number of space-saving tricks to produce a network which has 50 times less parameters.
Courbariaux and Bengio further demonstrate that it is possible to constrain a network entirely to binary weights and activations (either $+1$ or $-1$) without significant loss in accuracy.
Using this method, they are able to construct a convolutional neural network, and optimize it for CPU performance to achieve competitive results.
These results are largely corroborated by Rastegari et al. \cite{rastegari2016xnor}, who also use a binarized network and significant usage of the XNOR operation to optimize a wider variety of modern networks.
It is important to note, however, that the performance of these methods is still insufficient to reliably overtake GPU networks.
Courbariaux and Bengio perform their training against ``an unoptimized GPU kernel'', while Rastegari et al. perform an efficiency investigation but do not discuss raw performance.
While such approaches hint at future promise, in their current state they are more complicated and are still far from seeing general use in modern libraries.

On the other hand, it is not necessary to impose such harsh limits on the network in order to find areas of improvement.
Google's Inception network \cite{szegedy2015going}, developed by Szegedy et al., has gone through various iterations, which all involve complex pooling of different convolutional kernel sizes.
In their 2016 update to the architecture \cite{szegedy2016rethinking}, they focus on tuning the inefficiently large filter sizes used in the previous revision.
They note that a $5\times 5$ convolution is effectively the same (covers the same area) as two $3\times 3$ convolutions while requiring more parameters (25 versus $9 \cdot 2 = 18$), dubbing this reduction as filter factorization.
In the same vein, it is possible to reduce a $3\times 3$ convolution to a $3\times 1$ convolution followed by a $1\times 3$ convolution, which requires a third less parameters.

There are various benefits to an increased number of smaller layers over one larger layer beyond parameter reduction.
Reducing the number of layers allows for the increased application of nonlinear activation functions, which are generally regarded as critical for learning complex problems.
Furthermore, it allows an increased number of layers with the same number of parameters.
Most networks are primarily limited by memory, especially as modern training algorithms require a 
Even though inference is generally more efficient, it can still remain a difficult problem for more constrained hardware; part of Squeezenet's contribution was the possibility of reducing a model to a size that could be run on modern FPGAs.

\section{Network Expansion}
The counterpart to parameter deletion is network expansion, which tries to add parameters and learning capacity to a network dynamically.
One of the important works in this field is Cascade-Correlation Learning by Fahlman and Lebiere \cite{fahlman1990cascade}, which fixes a network and gradually adds single nodes to the network at a time.
Their proposed network learns without backpropagation but rather by adding new nodes in order to deal with error.
We derive some inspiration for our algorithm in Fahlman and Lebiere's interesting link between introduction of learning capacity and freezing the original network.
The main difficulty with their specific algorithm is that it depends on adding individual nodes, which would be prohibitively slow to generate the network sizes that are common in the modern age.
Additionally, we do not know if any work that utilizes their findings in a way that is able to take advantage of convolutions.

An alternative way of thinking about this problem was tackled by Chen et al. They develop Net2Net \cite{chen2015net2net}, which takes a pretrained smaller network and allows a partial transfer of these learned weights to an expanded network.
Noting that modern deep learning research usually requires the training of a number of different networks, they develop an architecture to minimize the amount of repeated computation.
They are able to achieve improved results on the ImageNet dataset.
In general, network expansion is an understudied topic in the modern literature, and we believe that there is significant room for improvement, especially with regards to modern deep networks.
We aim to provide a form of this by introducing dynamic network capacity, which allows layers to resize up to a fixed size during runtime.

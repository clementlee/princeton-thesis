\chapter{Discussion}
Our results are highly promising, and additionally show some other tendencies, which we highlight in this chapter.
Our algorithm appears to work as a regularizer, ensuring that a network does not devolve into a suboptimal state.
We are also able to see an improvement in generalization compared to standard training regimes, which is a key benefit.
Finally, we list a number of limitations and provide guidance and direction for potential future work.

\section{Regularization}

An important aspect of fixing part of the network capacity is that it prevents the network from diverging significantly.
This is, in effect, a form of regularization, which we can see most clearly in the function regression results.
By fixing the majority of the network, the capability of the network to produce noisy results is far more limited.
This may be an important property even if the network is unable to achieve significant improvements on a dataset, stability is an important goal of any training algorithm.
This stability may also allow the algorithm to perform better, as the moving average becomes less susceptible to noise.
We believe that this virtuous cycle can be further exploited by a more advanced extrapolation of the error curve.
Anecdotal results have indicated that the algorithm does indeed pick more opportune times to expand the network as the fixed capacity increases (and with it, the strength of the regularization), but further work would have be done to verify this effect.

Furthermore, this corroborates the known literature that network capacity is being used inefficiently.
The ability for a network to function well despite only being able to train on a fraction of its capacity indicates a potential overparametrization of the original network.
It would be interesting to apply parameter deletion methods to the frozen capacity, as they generally try to involve minimal perturbation to the network.
This would allow an efficient network to be constructed in-place, without requiring a significant amount of retraining.
We had previously attempted a version of the algorithm that gradually unfreezes the network as an attempt to improve late-stage error, but were unable to detect any major differences between this algorithm and standard training.
This indicates that the retraining process during most parameter deletion methods may be unnecessarily noisy, and we believe that our fixed capacity may help solve this problem.
By utilizing extra capacity to correct for and smooth the errors of the fixed portion, the network is given what is potentially a simpler problem.
We note that this is different from boosting or ensemble architectures due to the high degree of interconnection---as capacity is introduced, it is fully connected to all of the available capacity of the previous and next layers.
This means that the learning is far more organized as a single unit rather than as small substructures.

\section{Generalization}
As we noted briefly in the experiments, one of the interesting trends of the adaptive network was that it tended to overfit less, even if the full results were not as good.
Interestingly, its generalization performance was better than the standard network at the same training error, indicating that it had potentially learned the problem better under a certain metric.
Part of the hope with the algorithm is inspired by the idea that using less parameters prevents overfitting and allows for a model to be more general; this seems to be borne out by the results.
Through various versions of the algorithm, we were able to perform improvements to the testing error consistently; this is despite the standard network having fully utilized the training set, reaching 0\% error.
Our methods have shown consistent improvement, and most interestingly, our final CIFAR-100 test has lower training accuracy but higher testing accuracy than the standard network.

This is an interesting improvement, as the imperfect training accuracy may indicate that further gains could be made on both accuracies, perhaps by better informing the optimizer.
Crucially, the final fully-connected layer's capacity is currently modified along with the last convolutional layer, which could significantly impact the outputs.
In keeping with modern trends that focus on maximizing convolutions throughout the network, residual networks do not depend significantly on fully-connected layers. 
However, we believe that for the purposes of this algorithm, our results demonstrate the potential need for additional capacity that does not change as often.
Along with our good generalization results, performing further higher-level architectural optimizations may help improve results even further. 

\section{Limitations and Future Work}
There are number of topics that were unfortunately outside the realm of reasonable exploration during the course of this thesis.
Many of these pertain to the specifics of our algorithm, which could see significant fine tuning.
While our results are generally good, we believe that that are still major gains to be achieved by continuing along the same directions established by our work.
This thesis provides an interesting result, but also opens up a variety of questions for future investigation.

As noted, whenever possible, we have preferred to maximally utilize the currently available methods rather than performing significant rewrites specific to our problem.
This helped our work maintain its focus on the specifics of improving deep learning training, rather than work on significant reimplementations that would likely introduce new bugs into the system.
At the same time, this could be an area of future work, as we have briefly discussed the potential limitations of using off-the-shelf initializers and optimizers.
These restrictions are generally due to the lack of knowledge within the system, which could significant effects on the training capability of the network.
We noted that our initial sine function approximation network began to learn far slower than expected, which we conjectured to be due to poor initialization.
Furthermore, we were not able to independently verify whether the fixed weights were entirely frozen.
Due to the intracacies of the optimizer, it is possible that momentum terms, or just other modifications from standard gradient descent led our network to keep changing even when it was fixed.
We expect that Tensorflow's ability to stop gradients from flowing through the fixed sections should have effectively done the job, but it may have taken a few more epochs.
In order to solve these issues, we would have to perform custom implementations of Adam, Xavier, and perhaps other components as well that are aware of the capacity limitations we impose.
This would likely complicate the codebase significantly, as Tensorflow operations are not as easy to develop.
Some algorithmic questions also remain, such as the correct initialization values for added capacity.
These would involve significant new testing and theory to determine.

We also believe that sparsity is an interesting topic but were unable to cover it within this thesis.
In his work on spatially-sparse convolutional neural networks \cite{graham2014spatially}, Graham noted that there are potential improvements in architecture by performing sparse convolutions.
Tensorflow does not support such functionality at the moment, although it appears that they may be planning its development for the future \cite{spatiallysparseconv}.
For our experiments, we continue to rely on densely-connected convolutional layers.
Apart from the natural computational efficiency, we note that sparse networks are generally utilized in problems where the problem is seen as less compact or able to exploit the sparse connections---such is not often the case for image classification, which is our primary subject in this thesis.
We do note, however, that this would be a very interesting way of implementing dynamic network capacity that extends beyond our current implementation.
Importantly, this may allow the network to suffer less shock as additional capacity is added by initially minimizing the number of connections between the original or fixed section and the newly-added training section.
In this way, the sections can be trained somewhat like an ensemble of networks that gradually begins to learn some capacity for communication.
Therefore, controlling this dynamic would be an extremely powerful tool.

Another area of interest is the amount of dynamicism in the network architecture.
Srivastava et al. explore Highway Networks \cite{srivastava2015highway}, which allow learned connections to form between any two layers.
This is obviously a far more complex architecture, requiring additional connections in the network, but may be thought of as a higher-level abstraction over residual networks.
Allowing the network to develop not just in per-layer capacity but also in layer connections could allow better mutability.
The results regarding the training accuracy have shown that fixing significant capacity limits the ability of the network to overcome some of the errors caused by limitations of capacity.
However, it is possible that increased connections between each layer would provide the necessary adaptations to learn the problem well, without adding a significant number of parameters to the whole network.

While the hyperparameters for our algorithm were generally chosen on inspection of the testing baseline, we note that it may be possible to develop a reasonable set of defaults for an average user.
This would be highly beneficial, as it further removes the necessity of tuning.
Apart from edge cases which would be known to the user, it seems that basic analysis can indicate when convergence is beginning, and the algorithm can adjust accordingly.
A improved algorithm would perhaps entail a more detailed analysis of previous errors beyond a simple moving average, which would allow it to be smarter about when a resize is necessary, as opposed to occasionally falling for noise in the error.
Many small tweaks could be developed as a result of more extensive testing of the algorithm to better understand its behavior on a wider range of learning problems.

Another direction we see is in the potential for live user intervention during training.
In general, most modern methods do not allow any changes to the architecture, meaning that if certain parameters are set poorly but go unnoticed, significant time can be lost as the network will have to start training from scratch.
Technically, this functionality is available in a very crude sense in our current software, as the data is mostly saved into checkpoints that could be loaded and overwritten.
This means that by overwriting the current state variables of the algorithm, subsequent runs would then adopt the updated values.
Especially with Google's Tensorboard software, which allows a Tensorflow network to show its computational graph, log various properties, and much more, we see the potential for users to gradually tune a network on-the-fly.
In conjunction with our algorithm providing suggestions on network changes, it would be interesting to allow a more technical user to query specific statistics about the network, then make decisions on tuning without necessitating a new and costly training cycle.
This kind of fine-grained architectural control during runtime is completely new to the literature, so we see our work as a foundational first step.
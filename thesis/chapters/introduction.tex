\chapter{Introduction}

In recent years, deep learning has exploded as the forefront of what the New York Times has branded the ``A.I. Awakening''.
It has seen applications in nearly every field of artifical intelligence, and has grown to encompass far more as well.
Nearly every application of artifical intelligence has started to adopt deep learning, from classifying different kinds of whales to generating imitation C\'{e}zannes.
Deep learning is starting to surpass humans in a variety of complex classification tasks.
The generational improvements resulting from deep learning  rival the improvements made by traditional machine learning methods over a much longer period of time \cite{aiawakening}.

Furthermore, the advent of deep learning has been ushered in at a pace that has surprised even experts within the field.
Traditionally considered to be an unassailable human stronghold, AlphaGo \cite{silver2016mastering} was able to beat modern Go masters.
Deep learning has often been seen as an incredibly effective modelling tool, as it involves significantly less manual instruction than other methods of artificial intelligence.
Part of the allure of is the potential to have it understand high-level features without relying on domain knowledge, and in fact many deep learning results have demonstrated superior performance to expert humans.
While deep networks have not, in general, pointed to significant developments in universal AI, they are quickly becoming the standard method for specialized tasks.


Deep learning has come with additional complexities, however.
Typical deep networks, while providing excellent accuracy, have far worse computational efficiency than other methods of machine learning.
AlphaGo was reliant on large-scale distributed computing infrastructure in order to achieve peak performance, and in fact the modern-day superiority of deep networks has often been attributed to a maturity of hardware and technology.
Furthermore, due to the high computational costs of deep networks, developing new architectures is a very slow and costly process involving long wait times.
Combined with a generally confusing literature on ever-changing best practices, deep learning is quickly becoming a research quagmire.

To address these deficiencies, we aim to improve on the current state of network design by making the training process more transparent to the user.
Our methods promote stability in the network and improve generalizability of the network's output.
Across multiple datasets, we are able to see solid improvements which additionally show significant future promise.
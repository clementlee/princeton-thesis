\chapter{Introduction}

Deep learning has exploded in recent years as the forefront of what the New York Times has referred to as the ``A.I. Awakening.''

\emph{ < a whole lot more to go here > }
%fuck i dunno what to say next

AlphaGo shocked experts worldwide when, in a best of 7 series against 9-dan Go master Lee Sedol, it was able to win 4-1.
Go had often been seen as an example of a problem where humans had an unsurmountable advantage.
In the modern day, chess programs are able to brute force enough possibilities on a typical consumer laptop to defeat professionals.
However, Go has a far more complex move space: an cursory estimate indicates that there are on the order of $19!$ possible games.
Deep learning proved capable of solving the problem of judging the quality of a Go board position, which was a groundbreaking result in a field where even the best programs were unable to challenge low-ranked professionals.
Surprisingly, the developers of AlphaGo noted that ``AlphaGo evaluted thousands of times fewer positions than Deep Blue did in its chess match against Kasparov.'' \cite{silver2016mastering}

The promise of deep learning has come with additional complexities, however.
Typical deep networks, while providing excellent accuracy, have far worse computational efficiency than other methods of machine learning.
AlphaGo was reliant on large-scale distributed computing infrastructure in order to achieve peak performance, and in general, 

\emph{ < discuss performance concerns with deep learning and therefore the motivation for this thesis by optimizing that > }

% mention alphago, 

\chapter{Conclusion}
% brief summary
This thesis introduces the concept of dynamic network capacity, which allows for the fine-grained tuning of layer capacities.
No work in the literature has done this before.
We develop a training algorithm to utilize this structure, and combine it with fixed networks to encourage additional learning.
Our algorithm leads to large improvements on all of the the datasets we test on, and we observe potentially significant gains in efficiency.
Essentially, we improve on existing network architectures with minimal extra human intervention, and we believe that there are more accessible improvements in this direction that would even further maximize performance.
The success of our method allows us to abstract away the opaque hyperparameters involved in modern training and replace them with visible, understandable variables.

Deep learning is an extremely active area of development, and it is likely to proliferate in even more fields in the future.
Its recent explosion as a popular research field has resulted in a flurry of new knowledge being created, which requires better and more consistent training processes that are transparent to the user.
Our work can be applied to a wide range of existing network architectures, and is a significant boost to the architectural flexibility without a dramatic increase in complexity.
We believe our algorithm has far-reaching implications and future possibilities in making neural network models both easier to interface with and more efficient.
Our framework for working with dynamic network capacity pioneers a field of exploration that has been generally dormant in the literature, and we demonstrate that there are significant benefits to this problem in the modern day.

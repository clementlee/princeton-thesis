\chapter{Experiments and Results}
\section{Implementation}
We performed all of our experiments within Google's Tensorflow \cite{abadi2016tensorflow} framework.
Tensorflow imposes a style of computation which is not immediately adaptable to our experiments, but it was nevertheless chosen for its prevalence and impact on current methods.
Its popularity has largely affected the number of open-source code samples available, and many current architectures have clear examples in Tensorflow.
The thriving ecosystem of open-source contributions around Tensorflow proved to be a highly beneficial factor in providing a variety of existing architectures for experimentation.

As was noted before, Tensorflow operates in a slightly different way than many other libraries.
Rather than allowing the user to chain together operations at random, it fixes a computational graph which defines the full model.
Google's developers preferred this static model as it is generally well-suited to a lot of deep learning research, while also being flexible enough to allow distributed computing (of crucial importance to a a cloud company like Google).
This, however, poses an obvious problem with our algorithm, which is largely dynamic.
A few workarounds had to be developed in order to interface with the static computational graph.
While it is possible to use conditional blocks to disable parts of the graph, it is impossible to insert during runtime.
As such, the entire possible network capacity has to be allocated upfront, which potentially reduces the range of experimentation.
This additionally means that while parts of the network can be disabled, they still take important parameter space which cannot be reallocated to other parts of the network.

In his work on spatially-sparse convolutional neural networks \cite{graham2014spatially}, Graham noted that there are potential improvements in architecture by performing sparse convolutions.
Tensorflow does not support such functionality at the moment, although it appears that they may be planning its development for the future \cite{spatiallysparseconv}.
For our experiments, we continue to rely on densely-connected networks
% EXPLAIN PLS

Other libraries were explored briefly, but they either did not provide the necessary flexibility or have a reasonable set of tutorials/examples to facilitate the work here.
For example, one of the more common tools in imag e-based deep learning has been Caffe \cite{jia2014caffe}, which boasts well-tuned performance as well as a public repository of models in the Caffe Model Zoo.
Unfortunately, being written almost entirely in C++, it is largely unamenable to testing and infrastructure development.
Modifying Caffe to implement new training methods typically requires a significant contribution in C++, which requires an overhead not often undertaken except by researchers with significant prior experience.
Furthermore, models are loaded in a fixed format, which hampers the ability to dynamically redefine networks.
On the other side of the spectrum are libraries like Keras, which usually serve as a higher-level wrapper to other deep learning libraries.
They were generally judged as being insufficiently expressive for the type of modifications we performed, so we considered other options.

All experimentation was performed on a GTX 1060, which was provided via a grant from Princeton SEAS.
GPU computation has widely become the standard for deep learning computation in recent years, as it can provide nearly an order of magnitude in performance.
Particularly for models like modern residual networks, which can take days to converge to reasonable accuracy, it is nearly impossible to train neural networks on CPU servers.
Tensorflow still utilizes the CPU extensively to coordinate training and perform a significant amount of calculations, but modern-day GPUs are nearly perfectly designed for the type of computations required for convolutions.

A recent glut of libraries aimed at helping automate the deep learning deployment process has led to a variety of different methods.
NVIDIA's CUDA and CUDNN libraries, both of which re crucial for the performance of modern deep learning libraries, require a complex set of dependencies and installation procedures.
To automate this process, NVIDIA has recently released the \texttt{nvidia-docker} tool, which provides an abstraction on top of Docker that is designed to expose the GPU without requiring a complex installation method for the requisite GPU drivers.
We use this library to deploy CUDNN v5, as well as the latest drivers and Tensorflow version as of this writing.

\subsection{Experimental Details}
Throughout our experiments, we utilize the ADAM optimizer developed by Kingma and Ba \cite{kingma2014adam} as it allows adaptive training without requiring the careful learning rate tuning that is generally required for straightforward gradient-descent optimization.

We also rely on Glorot and Bengio's Xavier initializer \cite{glorot2010understanding}, as it is a common improvement over typical random initialization that remains relatively simple to use.


\section{Function Regression}
Our first experiment is a simplistic one, but one that is indicative of the basic algorithm's performance.
In this experiment, we approximate the trigonometric sine function in the range of $[-2\pi, 2\pi]$.
Our architecture for this experiment is extremely simple: it is merely a feedforward network with one hidden layer consisting of up to 500 nodes.
This is sufficient capacity to learn the sine function with great accuracy, but can still be heavily affected by the training regime applied to it.
We investigate the importance of the hidden layer's capacity by testing static networks of 100 and 500 nodes, then comparing these results to a dynamically sizing network.

For this experiment, we split the network into quarters, and initialize them all before any training begins.

\emph{ < Performance here has been very excellent overall. The results were in the presentation. > }




\section{MNIST Classifier}
Modern deep learning algorithms have generally tended to be developed for image classification purposes, in part due to the original usage of convolutional neural networks.
LeCun et al.'s original work was in designing a classifier for the MNIST dataset, a collection of handwritten digits 

\emph{ < As above, this was in my presentation. Results were good, but not as improved as the previous example. Also, it's important to note that small improvements to MNIST are pretty big because of high accuracy overall (99.32 vs 99.37 accuracy) > }


\section{CIFAR-100}
\emph{ < CIFAR-100 is a dataset with 10 classes, each with 10 subclasses (e.g. jets, then different kinds of jets). Results are coming up very soon hopefully! > }

\section{Performance}
We note that our algorithm involves nearly no overhead over the original architecture; a simple timing benchmark over 1000 epochs of MNIST indicates a performance difference of 3.5\% (37.9 seconds versus 36.6 seconds), which is well within the margin of error.
This is because all of the major time-consuming operations are performed on the GPU, to which our algorithm does not add significant stress.


Furthermore, by limiting the capacity of the network, we are able to achieve far faster initial training.
The initial timing experiment was performed by applying the algorithm but forcing it to use the full capacity of the network initially; this is far from the original intent.
By utilizing it in the same way as developed for the experiments, the first 1000 epochs of MNIST actually take 11.9 seconds.
While the algorithm takes more epochs to converge, the increased speed of working through the initial epochs is a significant boon.

\emph{ < Expanding on performance gains here because of the smaller trained network. Also, Tensorflow imposes some other interesting details here because of the array slicing necessary. > }
